{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:39:30.965825Z",
     "start_time": "2020-09-03T09:39:28.605717Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import random\n",
    "from six.moves import range\n",
    "# random.seed(5001)\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import pandas as pd\n",
    "from scipy.signal import decimate\n",
    "from scipy.stats import mode\n",
    "import pywt\n",
    "import seaborn as sn\n",
    "from collections import Counter\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4ae02363-68a4-4fc7-88a8-0760a60b77e7"
    }
   },
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:39:36.336459Z",
     "start_time": "2020-09-03T09:39:30.968509Z"
    },
    "nbpresent": {
     "id": "cba557b4-88e4-4514-b843-a9a10aa113e3"
    }
   },
   "outputs": [],
   "source": [
    "pa = pd.read_csv(\"Watch_accelerometer.csv\")\n",
    "# pa=pd.read_csv(\"Phones_gyroscope.csv\")\n",
    "# wa=pd.read_csv(\"Watch_accelerometer.csv\")\n",
    "# wg=pd.read_csv(\"Watch_gyroscope.csv\")\n",
    "print (pa.shape)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:39:38.418921Z",
     "start_time": "2020-09-03T09:39:36.339475Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pa = pa[pa['gt'] != 'null']\n",
    "print (pa.shape)\n",
    "pa = pa.dropna()\n",
    "print (pa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:39:38.781297Z",
     "start_time": "2020-09-03T09:39:38.421773Z"
    },
    "nbpresent": {
     "id": "345786db-e817-4950-94bb-2ca3a05a1758"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acts = pa['gt'].unique()\n",
    "users = pa['User'].unique()\n",
    "devices = pa['Device'].unique()\n",
    "print (devices)\n",
    "print (acts)\n",
    "print (users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:39:38.793888Z",
     "start_time": "2020-09-03T09:39:38.784973Z"
    },
    "nbpresent": {
     "id": "50079432-e9f9-4762-b957-b4d881f55fbc"
    }
   },
   "outputs": [],
   "source": [
    "l = {}\n",
    "for i, act in enumerate(acts):\n",
    "    l[act] = i\n",
    "print (l)\n",
    "\n",
    "dev = {}\n",
    "for i, d in enumerate(devices):\n",
    "    dev[d] = i\n",
    "print (dev)\n",
    "\n",
    "use = {}\n",
    "for i, u in enumerate(users):\n",
    "    use[u] = i\n",
    "print (use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> \n",
    "Train users: a, b, c, d, e, f, g, h\n",
    "\n",
    "Test user: i\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:39:38.862840Z",
     "start_time": "2020-09-03T09:39:38.796307Z"
    }
   },
   "outputs": [],
   "source": [
    "userTrain, userTest = [], []\n",
    "\n",
    "for i in range(len(users)):\n",
    "    train = []\n",
    "    \n",
    "    if i != 8:\n",
    "        train.extend(users[i+1:])\n",
    "    if i != 0:\n",
    "        train.extend(users[:i])\n",
    "    #print (users[i])\n",
    "    userTrain.append(train)\n",
    "    userTest.append(users[i])\n",
    "    \n",
    "print (userTrain, userTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:39:44.460997Z",
     "start_time": "2020-09-03T09:39:38.866101Z"
    },
    "nbpresent": {
     "id": "3807f2a7-eddd-4a14-bd7a-fafe02779c30"
    }
   },
   "outputs": [],
   "source": [
    "def getData(userList):\n",
    "\n",
    "    devs, acc, labels = [], [], []\n",
    "\n",
    "    for user in userList:\n",
    "        pa_user = pa[pa['User'] == user]\n",
    "        for act in acts:\n",
    "            pa_act = pa_user[pa_user['gt'] == act]\n",
    "            for device in devices:\n",
    "                pa_dev = pa_act[pa_act['Device'] == device]\n",
    "                pa_dev = pa_dev[['x', 'y','z']]\n",
    "                if str(device) == 'lgwatch_1' or str(device) == 'lgwatch_2':\n",
    "                    min_win = 400\n",
    "                # elif str(device) == 'gear_1' or str(device) == 'gear_2':\n",
    "                else:    \n",
    "                    min_win = 200\n",
    "                if(pa_dev.shape[0] >= min_win):\n",
    "                    acc.append(pa_dev.values)\n",
    "                    devs.append(device)\n",
    "                    labels.append(l[act])\n",
    "        print (len(labels))\n",
    "        print (f'{user} done')\n",
    "    \n",
    "    acc = np.array(acc)\n",
    "    labels = np.array(labels)\n",
    "    devs = np.array(devs)\n",
    "    print (\"Done\")\n",
    "    print(acc.shape, labels.shape, devs.shape)\n",
    "    return acc, labels, devs\n",
    "\n",
    "Xtrain1, Ytrain1, devs1 = getData(userTrain[-1])\n",
    "Xtest1, Ytest1, devs2 = getData(userTest[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "439cbde1-13d9-41b1-ab67-887ba5a49ee5"
    }
   },
   "source": [
    "# Getting the Windowed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:39:44.475571Z",
     "start_time": "2020-09-03T09:39:44.468089Z"
    },
    "nbpresent": {
     "id": "52c75edd-6a0f-4fa5-ad0b-c29b539fbe6d"
    }
   },
   "outputs": [],
   "source": [
    "def getWindowedData(acc, labels, index, w_min):\n",
    "    \n",
    "    windowData, windowLabels = [], []\n",
    "    num_windows = acc[index].shape[0] // w_min\n",
    "    if num_windows == 0:\n",
    "        print(acc[index].shape[0], w_min)\n",
    "    k = 0\n",
    "    for _ in range(num_windows):\n",
    "        windowData.append(acc[index][k:k+w_min])\n",
    "        k += w_min\n",
    "        windowLabels.append(labels[index])\n",
    "    return windowData, windowLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:39:44.609933Z",
     "start_time": "2020-09-03T09:39:44.480350Z"
    },
    "nbpresent": {
     "id": "6c89d9d8-dae6-4898-ad32-40d0b7917053"
    }
   },
   "outputs": [],
   "source": [
    "# Getting 2 seconds (200 samples) of data for all devices\n",
    "\n",
    "main_train = []\n",
    "for i in range(len(Xtrain1)):\n",
    "    if str(devs1[i]) == 'lgwatch_1' or str(devs1[i]) == 'lgwatch_2':\n",
    "        w_min = 400\n",
    "    else:\n",
    "        w_min = 200\n",
    "    main_train.append((getWindowedData(Xtrain1, Ytrain1, i, w_min)))\n",
    "\n",
    "main_test = []\n",
    "for i in range(len(Xtest1)):\n",
    "    if str(devs2[i]) == 'lgwatch_1' or str(devs2[i]) == 'lgwatch_2':\n",
    "        w_min = 400\n",
    "    else:\n",
    "        w_min = 200\n",
    "    main_test.append((getWindowedData(Xtest1, Ytest1, i, w_min)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:39:44.726997Z",
     "start_time": "2020-09-03T09:39:44.615093Z"
    }
   },
   "outputs": [],
   "source": [
    "print (main_train[-1][0][0].shape)\n",
    "print (main_train[-1][1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b300d560-c6df-49a3-8801-3b85c227f5ad"
    }
   },
   "source": [
    "# Decimating the Windowed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:06:33.126759Z",
     "start_time": "2018-11-26T15:06:33.009105Z"
    }
   },
   "outputs": [],
   "source": [
    "def decimateThatSignal(main, i):\n",
    "        final_decimated_signals = []\n",
    "        label = []\n",
    "        if main[i][0][0].shape[0] == 400:\n",
    "            for j in range(len(main[i][0])):\n",
    "                decimated_signal_0 = decimate(main[i][0][j][:,0], 2)\n",
    "                decimated_signal_1 = decimate(main[i][0][j][:,1], 2)\n",
    "                decimated_signal_2 = decimate(main[i][0][j][:,2], 2)\n",
    "                decimated_signal = np.dstack((decimated_signal_0, decimated_signal_1, decimated_signal_2))\n",
    "                final_decimated_signals.append(decimated_signal)\n",
    "                label.append(main[i][1][j])\n",
    "            return np.array(final_decimated_signals), np.array(label)\n",
    "        else:\n",
    "            return np.array(main[i][0]), np.array(main[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:07:00.446833Z",
     "start_time": "2018-11-26T15:06:33.128782Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_min = 200\n",
    "\n",
    "Xtrain1 = decimateThatSignal(main_train, 0)[0].reshape((-1, w_min, 3))\n",
    "Ytrain1 = decimateThatSignal(main_train, 0)[1]\n",
    "for i in range(1, len(main_train)):\n",
    "    print (i)\n",
    "    Xtrain1, Ytrain1 = np.vstack((Xtrain1, decimateThatSignal(main_train, i)[0].reshape((-1, w_min, 3)))), np.hstack((Ytrain1, decimateThatSignal(main_train, i)[1]))\n",
    "    #mainDecimated.append(DecimateThatSignal(i).reshape((-1, 100, 3)))\n",
    "Xtrain1 = np.array(Xtrain1)\n",
    "Ytrain1 = np.array(Ytrain1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:07:01.227938Z",
     "start_time": "2018-11-26T15:07:00.449012Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_min = 200\n",
    "\n",
    "Xtest1 = decimateThatSignal(main_test, 0)[0].reshape((-1, w_min, 3))\n",
    "Ytest1 = decimateThatSignal(main_test, 0)[1]\n",
    "for i in range(1, len(main_test)):\n",
    "    print (i)\n",
    "    Xtest1, Ytest1 = np.vstack((Xtest1, decimateThatSignal(main_test, i)[0].reshape((-1, w_min, 3)))), np.hstack((Ytest1, decimateThatSignal(main_test, i)[1]))\n",
    "    #mainDecimated.append(DecimateThatSignal(i).reshape((-1, 100, 3)))\n",
    "Xtest1 = np.array(Xtest1)\n",
    "Ytest1 = np.array(Ytest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:07:01.237995Z",
     "start_time": "2018-11-26T15:07:01.230516Z"
    }
   },
   "outputs": [],
   "source": [
    "print (Xtrain1.shape, Ytrain1.shape)\n",
    "print (Xtest1.shape, Ytest1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c922ca6f-84d0-4669-89dc-189928d8827a"
    }
   },
   "source": [
    "# Getting the DWTed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:07:01.313332Z",
     "start_time": "2018-11-26T15:07:01.242641Z"
    },
    "nbpresent": {
     "id": "733c86a8-d7f0-4775-a97c-d6ed1f90a25a"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pywt.wavelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:07:01.373676Z",
     "start_time": "2018-11-26T15:07:01.316872Z"
    },
    "nbpresent": {
     "id": "594027c7-3e29-4d49-bb5d-0e495b66f643"
    }
   },
   "outputs": [],
   "source": [
    "# Change window size here as and when DWT wavelet changes\n",
    "\n",
    "w_min = 100+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:07:01.654997Z",
     "start_time": "2018-11-26T15:07:01.376500Z"
    }
   },
   "outputs": [],
   "source": [
    "def performDWT(x, Y):\n",
    "    masterX = []\n",
    "    for i in range(len(x)):\n",
    "        Xca, Xda = pywt.dwt(x[i].reshape((-1, 3))[:,0], 'db4', mode='periodic')\n",
    "        Yca, Yda = pywt.dwt(x[i].reshape((-1, 3))[:,1], 'db4', mode='periodic')\n",
    "        Zca, Zda = pywt.dwt(x[i].reshape((-1, 3))[:,2], 'db4', mode='periodic')\n",
    "        coef = np.hstack((Xca, Yca, Zca)).reshape((-1, w_min, 3))\n",
    "        masterX.append((coef, Y[i]))\n",
    "        print (i)\n",
    "    masterX = np.array(masterX)\n",
    "    return masterX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:07:04.592720Z",
     "start_time": "2018-11-26T15:07:01.658225Z"
    }
   },
   "outputs": [],
   "source": [
    "masterTrain = performDWT(Xtrain1, Ytrain1)\n",
    "masterTest = performDWT(Xtest1, Ytest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:07:04.600036Z",
     "start_time": "2018-11-26T15:07:04.595718Z"
    }
   },
   "outputs": [],
   "source": [
    "print (masterTrain.shape, masterTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:22.220313Z",
     "start_time": "2018-11-26T15:09:22.179299Z"
    }
   },
   "outputs": [],
   "source": [
    "#for training\n",
    "Y_train = []\n",
    "X_train = np.zeros((masterTrain.shape[0], 1, w_min, 3))\n",
    "for i in range(masterTrain.shape[0]):\n",
    "    X_train[i, :, :] = masterTrain[i][0][:]\n",
    "    Y_train.append(masterTrain[i][1])\n",
    "y_train = np.array(Y_train)   \n",
    "\n",
    "\n",
    "#for test\n",
    "Y_test = []\n",
    "X_test = np.zeros((masterTest.shape[0], 1, w_min, 3))\n",
    "for i in range(masterTest.shape[0]):\n",
    "    X_test[i, :, :] = masterTest[i][0][:]\n",
    "    Y_test.append(masterTest[i][1])\n",
    "y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:22.933037Z",
     "start_time": "2018-11-26T15:09:22.928040Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:23.310137Z",
     "start_time": "2018-11-26T15:09:23.298467Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((-1, w_min, 3)).astype('float32')\n",
    "X_test = X_test.reshape((-1, w_min, 3)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:23.599080Z",
     "start_time": "2018-11-26T15:09:23.590808Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:24.301604Z",
     "start_time": "2018-11-26T15:09:24.296405Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 6\n",
    "img_rows, img_cols = 103, 3\n",
    "learning_rate = 2e-4\n",
    "# subset_size = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Valid-Pool Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:25.725050Z",
     "start_time": "2018-11-26T15:09:25.719375Z"
    }
   },
   "outputs": [],
   "source": [
    "np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:26.129507Z",
     "start_time": "2018-11-26T15:09:26.121550Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test, X_pool, y_test, y_pool = train_test_split(X_test, y_test, test_size=0.7, stratify=y_test)\n",
    "\n",
    "print ('Pool:', X_pool.shape, y_pool.shape)\n",
    "print ('Test:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Mean Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:28.242285Z",
     "start_time": "2018-11-26T15:09:28.132619Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train_All, X_test, y_train_All, y_test = train_test_split(data, labels, stratify=labels, test_size=0.2, random_state=5233)\n",
    "\n",
    "#X_train, X_test, y_train_All, y_test = train_test_split(data, labels, stratify=labels, test_size=0.2, random_state=5233)\n",
    "\n",
    "X_train_fit0 = StandardScaler().fit(X_train[:, :, 0])\n",
    "X_train0 = X_train_fit0.transform(X_train[:, :, 0])\n",
    "\n",
    "X_train_fit1 = StandardScaler().fit(X_train[:, :, 1])\n",
    "X_train1 = X_train_fit1.transform(X_train[:, :, 1])\n",
    "\n",
    "X_train_fit2 = StandardScaler().fit(X_train[:, :, 2])\n",
    "X_train2 = X_train_fit2.transform(X_train[:, :, 2])\n",
    "\n",
    "X_pool0 = X_train_fit0.transform(X_pool[:, :, 0])\n",
    "\n",
    "X_pool1 = X_train_fit1.transform(X_pool[:, :, 1])\n",
    "\n",
    "X_pool2 = X_train_fit2.transform(X_pool[:, :, 2])\n",
    "\n",
    "X_test0 = X_train_fit0.transform(X_test[:, :, 0])\n",
    "\n",
    "X_test1 = X_train_fit1.transform(X_test[:, :, 1])\n",
    "\n",
    "X_test2 = X_train_fit2.transform(X_test[:, :, 2])\n",
    "\n",
    "# print (X_train0.shape, X_test0.shape)\n",
    "\n",
    "X_train = np.dstack((X_train0, X_train1, X_train2))\n",
    "X_pool = np.dstack((X_pool0, X_pool1, X_pool2))\n",
    "X_test = np.dstack((X_test0, X_test1, X_test2))\n",
    "\n",
    "del X_train0, X_train1, X_train2, X_pool0, X_pool1, X_pool2, X_test0, X_test1, X_test2\n",
    "\n",
    "print (X_train.shape, X_pool.shape, X_test.shape)\n",
    "print (y_train.shape, y_pool.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:29.069470Z",
     "start_time": "2018-11-26T15:09:29.063054Z"
    }
   },
   "outputs": [],
   "source": [
    "print ('Distribution of Training Classes:', np.bincount(y_train))\n",
    "\n",
    "Y_train = to_categorical(y_train, num_classes)\n",
    "Y_test = to_categorical(y_test, num_classes)\n",
    "Y_pool = to_categorical(y_pool, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:29.813764Z",
     "start_time": "2018-11-26T15:09:29.808444Z"
    }
   },
   "outputs": [],
   "source": [
    "print ('Distribution of Train Classes:', np.bincount(y_train))\n",
    "print ('Distribution of Pool Classes:', np.bincount(y_pool))\n",
    "print ('Distribution of Test Classes:', np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:30.659357Z",
     "start_time": "2018-11-26T15:09:30.654434Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('Train:', X_train.shape, Y_train.shape)\n",
    "print ('Pool:', X_pool.shape, Y_pool.shape)\n",
    "print ('Test:', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T06:14:24.186633Z",
     "start_time": "2018-11-21T06:14:24.183033Z"
    }
   },
   "source": [
    "# Keras Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:31.619963Z",
     "start_time": "2018-11-26T15:09:31.615565Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Input, Flatten, Reshape, concatenate, Lambda\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from acquisition_functions import bald, varratio, maxentropy, random_acq, segnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:32.390992Z",
     "start_time": "2018-11-26T15:09:32.382507Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train0 = X_train[:, :, 0].reshape((-1, w_min, 1))\n",
    "X_train1 = X_train[:, :, 1].reshape((-1, w_min, 1))\n",
    "X_train2 = X_train[:, :, 2].reshape((-1, w_min, 1))\n",
    "\n",
    "X_pool0 = X_pool[:, :, 0].reshape((-1, w_min, 1))\n",
    "X_pool1 = X_pool[:, :, 1].reshape((-1, w_min, 1))\n",
    "X_pool2 = X_pool[:, :, 2].reshape((-1, w_min, 1))\n",
    "\n",
    "X_test0 = X_test[:, :, 0].reshape((-1, w_min, 1))\n",
    "X_test1 = X_test[:, :, 1].reshape((-1, w_min, 1))\n",
    "X_test2 = X_test[:, :, 2].reshape((-1, w_min, 1))\n",
    "\n",
    "print (X_train0.shape, X_pool0.shape, X_test0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:33.598343Z",
     "start_time": "2018-11-26T15:09:33.586082Z"
    }
   },
   "outputs": [],
   "source": [
    "def harnet():\n",
    "\n",
    "    # Model X\n",
    "    inputX = Input(shape=(X_train0.shape[1], X_train0.shape[2]))\n",
    "\n",
    "    convX1 = Conv1D(filters=8, kernel_size=2, padding='same', activation='relu')(inputX)\n",
    "    batchX1 = BatchNormalization()(convX1)\n",
    "    poolX1 = MaxPooling1D(pool_size=2, padding='same')(batchX1)\n",
    "\n",
    "    convX2 = Conv1D(filters=16, kernel_size=2, padding='same', activation='relu')(poolX1)\n",
    "    batchX2 = BatchNormalization()(convX2)\n",
    "    poolX2 = MaxPooling1D(pool_size=2, padding='same')(batchX2)\n",
    "\n",
    "    modelX = Flatten()(poolX2)\n",
    "\n",
    "    # Model Y\n",
    "    inputY = Input(shape=(X_train1.shape[1], X_train2.shape[2]))\n",
    "\n",
    "    convY1 = Conv1D(filters=8, kernel_size=2, padding='same', activation='relu')(inputY)\n",
    "    batchY1 = BatchNormalization()(convY1)\n",
    "    poolY1 = MaxPooling1D(pool_size=2, padding='same')(batchY1)\n",
    "\n",
    "    convY2 = Conv1D(filters=16, kernel_size=2, padding='same', activation='relu')(poolY1)\n",
    "    batchY2 = BatchNormalization()(convY2)\n",
    "    poolY2 = MaxPooling1D(pool_size=2, padding='same')(batchY2)\n",
    "\n",
    "    modelY = Flatten()(poolY2)\n",
    "\n",
    "    # Model Z\n",
    "    inputZ = Input(shape=(X_train2.shape[1], X_train2.shape[2]))\n",
    "\n",
    "    convZ1 = Conv1D(filters=8, kernel_size=2, padding='same', activation='relu')(inputZ)\n",
    "    batchZ1 = BatchNormalization()(convZ1)\n",
    "    poolZ1 = MaxPooling1D(pool_size=2, padding='same')(batchZ1)\n",
    "\n",
    "    convZ2 = Conv1D(filters=16, kernel_size=2, padding='same', activation='relu')(poolZ1)\n",
    "    batchZ2 = BatchNormalization()(convZ2)\n",
    "    poolZ2 = MaxPooling1D(pool_size=2, padding='same')(batchZ2)\n",
    "\n",
    "    # Merge Models X, Y, Z\n",
    "    modelZ = Flatten()(poolZ2)\n",
    "\n",
    "    merged_model = concatenate([modelX, modelY, modelZ])\n",
    "    print (K.int_shape(merged_model))\n",
    "\n",
    "    final_merge = Reshape((K.int_shape(merged_model)[1]//3, 3, 1))(merged_model)\n",
    "    print (K.int_shape(final_merge))\n",
    "\n",
    "    # Conv2D\n",
    "    conv1 = Conv2D(filters=8, kernel_size=(3, 3), padding='same')(final_merge)\n",
    "    batch1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2), padding='same')(batch1)\n",
    "\n",
    "    conv2 = Conv2D(filters=16, kernel_size=(3, 3), padding='same')(pool1)\n",
    "    batch2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2), padding='same')(batch2)\n",
    "\n",
    "    #drop2d = Lambda(lambda x: K.dropout(x, level=0.25))(pool2)\n",
    "\n",
    "    flatten = Flatten()(pool2)\n",
    "\n",
    "    # Dense\n",
    "    fc1 = Dense(16, activation='relu', kernel_initializer='glorot_normal')(flatten)\n",
    "    # fc1 = Dropout(0.25)(fc1)\n",
    "    # Stochastic Dropout Layer\n",
    "    fc1 = Lambda(lambda x: K.dropout(x, level=0.25))(fc1)\n",
    "    fc2 = Dense(8, activation='relu')(flatten)\n",
    "\n",
    "    # Output Layer - Softmax\n",
    "    output = Dense(num_classes, activation='softmax')(fc2)\n",
    "    \n",
    "    # Final Model\n",
    "    model = Model([inputX, inputY, inputZ], output)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy']) #beta_1=0.9, beta_2=0.999))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:36.273060Z",
     "start_time": "2018-11-26T15:09:35.019662Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = harnet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:36.279276Z",
     "start_time": "2018-11-26T15:09:36.276217Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save_path = './Save_Files/Watch_User_i.h5'\n",
    "# checkpoint = ModelCheckpoint(save_path, monitor='val_acc', save_best_only=True, mode='max')\n",
    "# callback = [checkpoint]\n",
    "\n",
    "# hist = model.fit([X_train0, X_train1, X_train2], Y_train, epochs=50, batch_size=64, \\\n",
    "#                  validation_data=([X_test0, X_test1, X_test2], Y_test))#, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:43.103628Z",
     "start_time": "2018-11-26T15:09:36.281881Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model('./Save_Files/Watch_User_i.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:43.117266Z",
     "start_time": "2018-11-26T15:09:43.106227Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save('./Save_Files/hist_Watch_User_i', hist.history)\n",
    "histC = np.load('./Save_Files/hist_Watch_User_i.npy').tolist()\n",
    "print (len(histC['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:47.053516Z",
     "start_time": "2018-11-26T15:09:47.018509Z"
    }
   },
   "outputs": [],
   "source": [
    "# print (max(hist.history['val_acc']))\n",
    "print (max(histC['val_acc']))\n",
    "model.evaluate([X_test0, X_test1, X_test2], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:51.015913Z",
     "start_time": "2018-11-26T15:09:50.998044Z"
    }
   },
   "outputs": [],
   "source": [
    "y_prob = model.predict([X_test0, X_test1, X_test2])\n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "\n",
    "y_true = np.array([np.argmax(y) for y in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:51.589380Z",
     "start_time": "2018-11-26T15:09:51.583775Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (y_true.shape, y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess for Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:53.171915Z",
     "start_time": "2018-11-26T15:09:53.161431Z"
    }
   },
   "outputs": [],
   "source": [
    "subset_size = X_pool.shape[0]\n",
    "\n",
    "subset_indices = np.asarray(random.sample(range(0, X_pool.shape[0]), subset_size))\n",
    "X_pool_subset = X_pool[subset_indices]\n",
    "Y_pool_subset = Y_pool[subset_indices]\n",
    "# X_pool_new = np.delete(X_pool, subset_indices, axis=0)\n",
    "# Y_pool_new = np.delete(Y_pool, subset_indices, axis=0)\n",
    "\n",
    "print (X_pool_subset.shape, Y_pool_subset.shape)\n",
    "# print (X_pool_new.shape, Y_pool_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:09:57.506332Z",
     "start_time": "2018-11-26T15:09:57.499779Z"
    }
   },
   "outputs": [],
   "source": [
    "X_pool_subset0 = X_pool_subset[:, :, 0].reshape((-1, w_min, 1))\n",
    "X_pool_subset1 = X_pool_subset[:, :, 1].reshape((-1, w_min, 1))\n",
    "X_pool_subset2 = X_pool_subset[:, :, 2].reshape((-1, w_min, 1))\n",
    "\n",
    "print (X_pool_subset0.shape, X_pool_subset1.shape, X_pool_subset2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:17:06.467304Z",
     "start_time": "2018-11-26T15:17:06.461128Z"
    }
   },
   "outputs": [],
   "source": [
    "pool_min = 0\n",
    "pool_max = int(0.6*X_pool_subset.shape[0])\n",
    "pool_iter = pool_max//8\n",
    "\n",
    "print (pool_min, pool_max, pool_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:17:26.555134Z",
     "start_time": "2018-11-26T15:17:26.550224Z"
    }
   },
   "outputs": [],
   "source": [
    "pool_percents = np.arange(12.5, 101, 12.5)\n",
    "print (pool_percents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:24:39.102398Z",
     "start_time": "2018-11-26T15:20:17.145662Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_inc_bald, f1_inc_bald, recall_inc_bald = [], [], []\n",
    "\n",
    "# Acquisition queries 8 times\n",
    "for n_queries in range(pool_min, pool_max, pool_iter)[1:]:\n",
    "    \n",
    "    print ('Queries:', n_queries)\n",
    "    model = load_model('./Save_Files/Watch_User_i.h5')\n",
    "\n",
    "    bald_uncertainty_estimates = bald([X_pool_subset0, X_pool_subset1, X_pool_subset2], \\\n",
    "                                      num_classes, model, 16, 10)\n",
    "\n",
    "    bald_uncertainty_estimates = bald_uncertainty_estimates.flatten()\n",
    "    acquired_indices = bald_uncertainty_estimates.argsort()[-n_queries:][::-1]\n",
    "\n",
    "    X_acquired_bald = X_pool[acquired_indices]\n",
    "    Y_acquired_bald = Y_pool[acquired_indices]\n",
    "\n",
    "    print ('BALD Acquired Shape:', X_acquired_bald.shape, Y_acquired_bald.shape)\n",
    "\n",
    "    y_acquired_bald = Y_acquired_bald.argmax(axis=-1)\n",
    "    np.bincount(y_acquired_bald)\n",
    "\n",
    "    # Incremental Learning\n",
    "    X_acquired0 = X_acquired_bald[:, :, 0].reshape((-1, w_min, 1))\n",
    "    X_acquired1 = X_acquired_bald[:, :, 1].reshape((-1, w_min, 1))\n",
    "    X_acquired2 = X_acquired_bald[:, :, 2].reshape((-1, w_min, 1))\n",
    "\n",
    "    print (X_acquired0.shape, X_acquired1.shape, X_acquired2.shape)\n",
    "\n",
    "    hist = model.fit([X_acquired0, X_acquired1, X_acquired2], Y_acquired_bald, epochs=30, batch_size=4, \\\n",
    "              validation_data=([X_test0, X_test1, X_test2], Y_test))\n",
    "    inc_acc = max(hist.history['val_acc'])\n",
    "    model.evaluate([X_test0, X_test1, X_test2], Y_test, batch_size=8)\n",
    "    print ('Incremental Learning:', inc_acc)\n",
    "    \n",
    "    acc_inc_bald.append(inc_acc)\n",
    "    y_prob = model.predict([X_test0, X_test1, X_test2], batch_size=8)\n",
    "    y_pred = y_prob.argmax(axis=-1)\n",
    "    f1_inc_bald.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    recall_inc_bald.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Var Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:34:21.219501Z",
     "start_time": "2018-11-26T15:27:59.846272Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_inc_var, f1_inc_var, recall_inc_var = [], [], []\n",
    "\n",
    "# Acquisition queries 8 times\n",
    "for n_queries in range(pool_min, pool_max, pool_iter)[1:]:\n",
    "    \n",
    "    print ('Queries:', n_queries)\n",
    "    model = load_model('./Save_Files/Watch_User_i.h5')\n",
    "\n",
    "    var_uncertainty_estimates = varratio([X_pool_subset0, X_pool_subset1, X_pool_subset2], \\\n",
    "                                      num_classes, model, 16, 10)\n",
    "\n",
    "    var_uncertainty_estimates = var_uncertainty_estimates.flatten()\n",
    "    acquired_indices = var_uncertainty_estimates.argsort()[-n_queries:][::-1]\n",
    "\n",
    "    X_acquired_var = X_pool[acquired_indices]\n",
    "    Y_acquired_var = Y_pool[acquired_indices]\n",
    "\n",
    "    print ('Var Ratio Acquired Shape:', X_acquired_var.shape, Y_acquired_var.shape)\n",
    "\n",
    "    y_acquired_var = Y_acquired_var.argmax(axis=-1)\n",
    "    np.bincount(y_acquired_var)\n",
    "\n",
    "    # Incremental Learning\n",
    "    X_acquired0 = X_acquired_var[:, :, 0].reshape((-1, w_min, 1))\n",
    "    X_acquired1 = X_acquired_var[:, :, 1].reshape((-1, w_min, 1))\n",
    "    X_acquired2 = X_acquired_var[:, :, 2].reshape((-1, w_min, 1))\n",
    "\n",
    "    print (X_acquired0.shape, X_acquired1.shape, X_acquired2.shape)\n",
    "\n",
    "    hist = model.fit([X_acquired0, X_acquired1, X_acquired2], Y_acquired_var, epochs=30, batch_size=4, \\\n",
    "              validation_data=([X_test0, X_test1, X_test2], Y_test))\n",
    "    inc_acc = max(hist.history['val_acc'])\n",
    "    model.evaluate([X_test0, X_test1, X_test2], Y_test, batch_size=8)\n",
    "    print ('Incremental Learning:', inc_acc)\n",
    "    \n",
    "    acc_inc_var.append(inc_acc)\n",
    "    y_prob = model.predict([X_test0, X_test1, X_test2], batch_size=8)\n",
    "    y_pred = y_prob.argmax(axis=-1)\n",
    "    f1_inc_var.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    recall_inc_var.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:44:14.032032Z",
     "start_time": "2018-11-26T15:36:18.859679Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_inc_maxent, f1_inc_maxent, recall_inc_maxent = [], [], []\n",
    "\n",
    "# Acquisition queries 8 times\n",
    "for n_queries in range(pool_min, pool_max, pool_iter)[1:]:\n",
    "    \n",
    "    print ('Queries:', n_queries)\n",
    "    model = load_model('./Save_Files/Watch_User_i.h5')\n",
    "\n",
    "    maxent_uncertainty_estimates = maxentropy([X_pool_subset0, X_pool_subset1, X_pool_subset2], \\\n",
    "                                      num_classes, model, 16, 10)\n",
    "\n",
    "    maxent_uncertainty_estimates = maxent_uncertainty_estimates.flatten()\n",
    "    acquired_indices = maxent_uncertainty_estimates.argsort()[-n_queries:][::-1]\n",
    "\n",
    "    X_acquired_maxent = X_pool[acquired_indices]\n",
    "    Y_acquired_maxent = Y_pool[acquired_indices]\n",
    "\n",
    "    print ('Max Entropy Acquired Shape:', X_acquired_maxent.shape, Y_acquired_maxent.shape)\n",
    "\n",
    "    y_acquired_maxent = Y_acquired_maxent.argmax(axis=-1)\n",
    "    np.bincount(y_acquired_maxent)\n",
    "\n",
    "    # Incremental Learning\n",
    "    X_acquired0 = X_acquired_maxent[:, :, 0].reshape((-1, w_min, 1))\n",
    "    X_acquired1 = X_acquired_maxent[:, :, 1].reshape((-1, w_min, 1))\n",
    "    X_acquired2 = X_acquired_maxent[:, :, 2].reshape((-1, w_min, 1))\n",
    "\n",
    "    print (X_acquired0.shape, X_acquired1.shape, X_acquired2.shape)\n",
    "\n",
    "    hist = model.fit([X_acquired0, X_acquired1, X_acquired2], Y_acquired_maxent, epochs=30, batch_size=4, \\\n",
    "              validation_data=([X_test0, X_test1, X_test2], Y_test))\n",
    "    inc_acc = max(hist.history['val_acc'])\n",
    "    model.evaluate([X_test0, X_test1, X_test2], Y_test, batch_size=8)\n",
    "    print ('Incremental Learning:', inc_acc)\n",
    "    \n",
    "    acc_inc_maxent.append(inc_acc)\n",
    "    y_prob = model.predict([X_test0, X_test1, X_test2], batch_size=8)\n",
    "    y_pred = y_prob.argmax(axis=-1)\n",
    "    f1_inc_maxent.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    recall_inc_maxent.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Acquisitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:56:09.168877Z",
     "start_time": "2018-11-26T15:46:32.432984Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_inc_rand, f1_inc_rand, recall_inc_rand = [], [], []\n",
    "\n",
    "# Acquisition queries 8 times\n",
    "for n_queries in range(pool_min, pool_max, pool_iter)[1:]:\n",
    "    \n",
    "    print ('Queries:', n_queries)\n",
    "    model = load_model('./Save_Files/Watch_User_i.h5')\n",
    "\n",
    "    rand_uncertainty_estimates = random_acq([X_pool_subset0, X_pool_subset1, X_pool_subset2], \\\n",
    "                                      num_classes, model, 16, 10)\n",
    "\n",
    "    rand_uncertainty_estimates = rand_uncertainty_estimates.flatten()\n",
    "    acquired_indices = rand_uncertainty_estimates.argsort()[-n_queries:][::-1]\n",
    "\n",
    "    X_acquired_rand = X_pool[acquired_indices]\n",
    "    Y_acquired_rand = Y_pool[acquired_indices]\n",
    "\n",
    "    print ('Random Acquired Shape:', X_acquired_rand.shape, Y_acquired_rand.shape)\n",
    "\n",
    "    y_acquired_rand = Y_acquired_rand.argmax(axis=-1)\n",
    "    np.bincount(y_acquired_rand)\n",
    "\n",
    "    # Incremental Learning\n",
    "    X_acquired0 = X_acquired_rand[:, :, 0].reshape((-1, w_min, 1))\n",
    "    X_acquired1 = X_acquired_rand[:, :, 1].reshape((-1, w_min, 1))\n",
    "    X_acquired2 = X_acquired_rand[:, :, 2].reshape((-1, w_min, 1))\n",
    "\n",
    "    print (X_acquired0.shape, X_acquired1.shape, X_acquired2.shape)\n",
    "\n",
    "    hist = model.fit([X_acquired0, X_acquired1, X_acquired2], Y_acquired_rand, epochs=60, batch_size=16, \\\n",
    "              validation_data=([X_test0, X_test1, X_test2], Y_test))\n",
    "    inc_acc = max(hist.history['val_acc'])\n",
    "    model.evaluate([X_test0, X_test1, X_test2], Y_test, batch_size=8)\n",
    "    print ('Incremental Learning:', inc_acc)\n",
    "    \n",
    "    acc_inc_rand.append(inc_acc)\n",
    "    y_prob = model.predict([X_test0, X_test1, X_test2], batch_size=8)\n",
    "    y_pred = y_prob.argmax(axis=-1)\n",
    "    f1_inc_rand.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    recall_inc_rand.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:56:33.622596Z",
     "start_time": "2018-11-26T15:56:33.176741Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_inc_df = pd.DataFrame(index=np.arange(pool_min, pool_max, pool_iter)[1:])\n",
    "\n",
    "acc_inc_df['BALD'] = np.array(acc_inc_bald) * 100\n",
    "acc_inc_df['VarRatio'] = np.array(acc_inc_var) * 100\n",
    "acc_inc_df['MaxEntropy'] = np.array(acc_inc_maxent) * 100\n",
    "acc_inc_df['Random'] = np.array(acc_inc_rand) * 100\n",
    "\n",
    "acc_inc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:56:38.878807Z",
     "start_time": "2018-11-26T15:56:38.859789Z"
    }
   },
   "outputs": [],
   "source": [
    "f1_inc_df = pd.DataFrame(index=np.arange(pool_min, pool_max, pool_iter)[1:])\n",
    "\n",
    "f1_inc_df['BALD'] = np.array(f1_inc_bald) * 100\n",
    "f1_inc_df['VarRatio_'] = np.array(f1_inc_var) * 100\n",
    "f1_inc_df['MaxEntropy'] = np.array(f1_inc_maxent) * 100\n",
    "f1_inc_df['Random'] = np.array(f1_inc_rand) * 100\n",
    "\n",
    "f1_inc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:59:30.386085Z",
     "start_time": "2018-11-26T15:59:30.364450Z"
    }
   },
   "outputs": [],
   "source": [
    "recall_inc_df = pd.DataFrame(index=np.arange(pool_min, pool_max, pool_iter)[1:])\n",
    "\n",
    "recall_inc_df['BALD'] = np.array(recall_inc_bald) * 100\n",
    "recall_inc_df['VarRatio_'] = np.array(recall_inc_var) * 100\n",
    "recall_inc_df['MaxEntropy'] = np.array(recall_inc_maxent) * 100\n",
    "recall_inc_df['Random'] = np.array(recall_inc_rand) * 100\n",
    "\n",
    "recall_inc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T15:59:36.383333Z",
     "start_time": "2018-11-26T15:59:36.058729Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_inc_df.to_csv('./CSV_Results/User_i/acc_inc.csv', sep=',')\n",
    "f1_inc_df.to_csv('./CSV_Results/User_i/f1_inc.csv', sep=',')\n",
    "recall_inc_df.to_csv('./CSV_Results/User_i/recall_inc.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
